<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>MedReason-Dx: Benchmarking Step-by-Step Reasoning of Language Models in Medical Diagnosis</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
        h1 { color: #2c3e50; }
        h2 { color: #34495e; }
        p { text-align: justify; }
        .abstract { background-color: #f0f0f0; padding: 20px; border-left: 5px solid #2980b9; }
    </style>
</head>
<body>
    <h1>MedReason-Dx: Benchmarking Step-by-Step Reasoning of Language Models in Medical Diagnosis</h1>
    <p><strong>Authors:</strong> MedReason-Dx Team</p>
    <div class="abstract">
        <h2>Abstract</h2>

      
        <p>In high-stakes domains like medicine, <strong>how</strong> an AI arrives at an answer can be as critical as the answer itself. 
        However, existing medical question answering benchmarks largely ignore the reasoning process, evaluating models only on final answer accuracy. 
        This paper addresses the overlooked importance of reasoning path evaluation in medical AI. We introduce <strong>MedReason-Dx</strong>, 
        a novel benchmark that assesses not just answers but the step-by-step reasoning behind them. MedReason-Dx provides expert-annotated 
        step-by-step solutions for both multiple-choice and open-ended questions, spanning 24 medical specialties. 
        By requiring models to produce and be evaluated on intermediate reasoning steps, our benchmark enables rigorous testing of interpretability 
        and logical consistency in medical QA. We present the design of MedReason-Dx and outline diverse evaluation metrics that reward faithful reasoning. 
        We hope this resource will advance the development of robust, interpretable medical decision support systems and foster research into large language 
        models that can reason as well as they respond.</p>
    </div>

    <h2>Introduction</h2>
    <p>Artificial intelligence systems for healthcare must not only deliver correct answers but also provide <em>justifiable reasoning</em>. In clinical decision support and medical question answering (QA), the reasoning path that leads to an answer is critical for trust and safety. A model that arrives at a diagnosis by flawed logic or guesswork poses significant risks, even if the final answer is correct. Conversely, a model that explains its reasoning can enable practitioners to verify each step, ensuring the conclusion is sound. Despite this, most existing benchmarks in medical AI evaluate models solely on whether the final answer is right, with little or no assessment of the reasoning process. This gap is problematic in high-stakes domains: evaluating only end answers may overlook dangerous reasoning errors and fails to encourage the development of models that ``think'' in a human-like, transparent manner. Recent advances in large language models (LLMs) and prompting techniques have brought reasoning to the forefront of AI research. In particular, \textit{chain-of-thought} (CoT) prompting has demonstrated that LLMs can generate step-by-step solutions to complex problems, from math and logic puzzles to medical questions. By prompting models to articulate intermediate steps, researchers have achieved improved performance on challenging tasks and gained insight into model decision-making. For example, state-of-the-art medical LLMs can now produce explanations or rationales alongside their answers, showcasing the potential of AI to handle intricate clinical reasoning. These developments underscore an urgent need for benchmarks that can evaluate not just final accuracy but the <em>quality of reasoning</em> LLMs employ. If a model is prompted to reason but we lack ground truth reasoning paths for comparison, we cannot rigorously assess whether the model’s reasoning is correct, complete, or clinically valid. Several medical QA datasets and benchmarks have emerged, yet they predominantly focus on answer correctness. Standard benchmarks drawn from medical exams (e.g., USMLE-style question banks, MedQA and MedMCQA) and research datasets like PubMedQA have driven progress in factual recall and question answering. Some of these resources include a short explanation or reference for the answer, but they do not provide a detailed, stepwise reasoning chain that could be used to evaluate a model’s thought process. In other words, existing benchmarks treat reasoning as an implicit skill, not an explicit target of evaluation. A model might earn full marks by selecting the correct option in a multiple-choice question, while in reality it could have arrived at that answer via incorrect assumptions or lucky guesswork. Conversely, a model might demonstrate mostly correct reasoning and make a minor error at the final step, but current benchmarks would simply mark the entire answer as wrong, offering no credit for nor analysis of the model’s reasoning ability. This limitation hampers the development of robust medical AI: it is difficult to discern whether improvements in accuracy are due to better reasoning or just better pattern matching, and it provides no incentive for models to output interpretable solutions. To address these challenges, we propose <strong>MedReason-Dx</strong>, a new benchmark expressly designed to evaluate chain-of-thought reasoning in medical question answering. MedReason-Dx (where “CoT” denotes Chain-of-Thought) introduces several key innovations to the evaluation of medical AI: </p>

<ul>
  <li><strong>Expert-annotated reasoning chains:</strong> Each question in MedReason-Dx is accompanied by a step-by-step solution path crafted by medical experts. These reasoning chains detail the logical steps required to arrive at the correct answer, including relevant clinical facts, intermediate inferences, and elimination of distractors in the case of multiple-choice items. This provides a gold-standard trace of correct reasoning against which model-generated solutions can be compared. </li>
  <li><strong>Diverse question formats and topics:</strong> Our benchmark covers a broad spectrum of medical knowledge. It includes both multiple-choice questions (with several answer options) and open-ended questions that require free-form answers, ensuring that models are tested on various response formats. The questions span 24 medical specialties, ranging from internal medicine and cardiology to pediatrics, surgery, and more. This diversity reflects the real-world breadth of medical practice and ensures that the benchmark evaluates reasoning across different sub-domains and problem types (diagnosis, treatment decisions, biomedical mechanism explanations, etc.). </li>
  <li><strong>Evaluation metrics for reasoning quality:</strong> MedReason-Dx departs from traditional single-metric evaluation by introducing multiple criteria to assess model performance. In addition to standard answer accuracy, we define metrics that measure the fidelity of a model’s reasoning to the expert-provided chain. For instance, we evaluate whether the model’s reasoning covers the same key steps or medical facts as the reference solution, and whether the logical progression is sound. This could involve step-wise accuracy scoring, similarity measures between generated and reference reasoning, and expert review of reasoning coherence. By quantifying reasoning quality, the benchmark encourages models that are not only correct, but \emph{correct for the right reasons}. </li>
  <li><strong>Interpretability and robustness focus:</strong> By requiring and evaluating intermediate reasoning, MedReason-Dx places interpretability at the core of model assessment. This is especially crucial for medical AI systems that clinicians need to trust. A model that can articulate a valid reasoning chain is inherently more transparent and easier to debug than one that only outputs an answer. Furthermore, focusing on reasoning helps reveal when a model’s knowledge is superficial. We anticipate that models performing well on MedReason-Dx will demonstrate greater robustness, as they must handle complex multi-step problems in a principled way rather than relying on shallow cues. Our benchmark thus serves as a stress test for genuine reasoning ability in medical contexts. </li>
</ul>  

    <h2>Related Works</h2>
    <h3>Medical LLMs</h3>
    <p>The evolution of medical large language models (Med-LLMs) has led to advancements in model architectures, training paradigms, and domain-specific adaptations, enabling applications in information extraction, clinical decision support, dialogue systems, and multimodal medical AI.

<p>Early Med-LLMs, such as BioBERT~\citep{lee2020biobert} and PubMedBERT~\citep{gu2021domain}, were trained on extensive biomedical literature and PubMed abstracts, excelling in tasks like named entity recognition, relation extraction, and text classification. Models such as ClinicalT5~\citep{lu2022clinicalt5} and GatorTron~\citep{yang2022gatortron} extend this capability to clinical text summarization and report generation, while Codex-Med~\citep{lievin2024can} specializes in structured medical documentation. Galactica~\citep{taylor2022galactica}, designed for scientific and medical applications, enhances literature analysis and information retrieval.</p>

<p>Recent models incorporate instruction fine-tuning (IFT) and reinforcement learning from human feedback (RLHF) to improve the accuracy of medical text generation and knowledge extraction. Med-PaLM~\citep{singhal2023large} and Med-PaLM 2~\citep{singhal2025toward} exemplify this trend, refining medical question answering and clinical decision-making. Med-Alpaca~\citep{han2023medalpaca} further demonstrates the adaptability of fine-tuned language models for specialized healthcare applications. Meanwhile, GatorTronGPT~\citep{peng2023study} builds on the GatorTron architecture with targeted fine-tuning, enhancing its precision in medical report generation. Conversational AI models like ChatDoctor~\citep{li2023chatdoctor} is tailored for virtual medical consultations, offering patient triage assistance and personalized recommendations.</p>

<p>Beyond these, several Med-LLMs focus on domain-specific adaptations. PMC-LLaMA~\citep{wu2023pmc} enhances biomedical literature processing, aiding both academic research and clinical applications. GPT-4-Med~\citep{nori2023capabilities}, a refined adaptation of GPT-4, excels in complex clinical text processing and high-quality medical content generation.
In the field of Traditional Chinese Medicine (TCM), models like Taiyi-LLM~\citep{luo2024taiyi}, and Zhongjing~\citep{yang2024zhongjing} integrate classical TCM literature with modern medical insights, supporting diagnosis and treatment planning. 
Additionally, advancements in multilingual and multimodal medical models have broadened AI's applicability in global healthcare. HuatuoGPT~\citep{zhang2023huatuogpt} and its successor HuatuoGPT-II~\citep{chen2023huatuogpt} leverage expanded datasets and optimized architectures to improve clinical report generation and diagnostic decision support. Med-Flamingo~\citep{moor2023med} extends Med-LLM capabilities to multimodal medical tasks, integrating textual and visual information. Med-Gemini~\citep{saab2024capabilities}, a bilingual model, facilitates cross-lingual medical communication, promoting international healthcare collaboration.</p>

<p>These advancements underscore the ongoing evolution of Med-LLMs, enhancing their ability to process complex medical language, integrate multimodal data, and support diverse healthcare applications. As these models continue to evolve, they hold the potential to significantly improve clinical decision-making, personalized medicine, and cross-cultural medical communication.</p>

<h3>Medical Benchmarks</h3>
<p>The development of diverse and standardized datasets, along with robust evaluation platforms, is essential for advancing AI applications in the medical domain. Existing research in this area can be broadly categorized into two main directions: (1) datasets tailored for various medical AI tasks and (2) automated benchmarks designed to assess the clinical capabilities of large models.</p>

<p>The first category consists of datasets that support tasks such as information extraction, question answering, text generation, and natural language inference. For instance, datasets like GENIA~\citep{kim2003genia}, CADEC~\citep{karimi2015cadec}, and BC5CDR~\citep{li2016biocreative} are widely used for named entity recognition, relation extraction, and event detection across biomedical literature and clinical records. Meanwhile, MedQA~\citep{jin2021disease}, PubMedQA~\citep{jin2019pubmedqa}, CMCQA~\citep{xia2022lingyi}, and Huatuo-26M~\citep{li2023huatuo} have been developed to evaluate models' abilities in medical knowledge retrieval, clinical reasoning, and diagnostic decision-making. Additionally, datasets such as MIMIC-III~\citep{johnson2016mimic}, MIMIC-CXR~\citep{johnson2019mimic}, HealthSearchQA~\citep{singhal2023large}, and CORD-19~\citep{wang2020cord} facilitate tasks like clinical report generation, summarization, and case-based discussions. In the natural language inference domain, MedNLI~\citep{romanov2018lessons} provides a benchmark for understanding logical relationships in medical texts.
Recently, MedReason~\citep{wu2025medreason} was proposed to address the scarcity of high-quality, step-by-step reasoning data in the medical domain. Unlike datasets distilled directly from general-purpose LLMs, MedReason constructs 32,682 question–answer pairs with detailed Chain-of-Thought explanations, leveraging a structured medical knowledge graph to extract and guide the reasoning paths. These reasoning chains are factually grounded and validated through both automated answer checking and expert review by medical professionals from diverse specialties.</p>

<p>The second category focuses on automated benchmarks for evaluating large medical models, reducing reliance on expert-driven manual assessments. MedBench~\citep{cai2024medbench} provides a broad evaluation platform with 40,041 questions covering various medical fields. AutoEval~\citep{liao2023automatic} reformats USMLE questions into multi-turn dialogues, assessing models based on information coverage and task accuracy. LLM-Mini-CEX~\citep{shi2023llm} leverages patient simulators and ChatGPT to evaluate diagnostic dialogue quality. MedGPTEval~\citep{xu2023medgpteval} integrates Chinese medical datasets and public benchmarks, using 16 expert-refined indicators to measure professional competence. LLM-Human Evaluation~\citep{chiang2023can} examines automated assessment feasibility, showing alignment with human evaluators in adversarial and open-ended tasks. These frameworks systematically measure model performance, lower assessment costs, and support medical AI optimization.</p>  

<h2>Table 1: Comparison with existing Medical QA benchmarks</h2>
<table border="1" cellspacing="0" cellpadding="5">
<thead>
<tr>
  <th>Benchmark</th><th>CoT Eval</th><th># Domains</th><th>Reasoning Intensive</th><th>MCQ</th><th>OEQ</th><th>Expert Annotation</th>
</tr>
</thead>
<tbody>
<tr><td>MMedBench</td><td>✘</td><td>21</td><td>✔</td><td>✔</td><td>✘</td><td>✔</td></tr>
<tr><td>MedQA</td><td>✘</td><td>-</td><td>✘</td><td>✔</td><td>✘</td><td>✘</td></tr>
<tr><td>MedMCQA</td><td>✘</td><td>21</td><td>✘</td><td>✔</td><td>✘</td><td>✔</td></tr>
<tr><td>MMLU</td><td>✘</td><td>6</td><td>✘</td><td>✔</td><td>✘</td><td>✘</td></tr>
<tr><td>Medbullets</td><td>✘</td><td>-</td><td>✔</td><td>✔</td><td>✘</td><td>✔</td></tr>
<tr><td>JAMA Challenge</td><td>✘</td><td>13</td><td>✔</td><td>✔</td><td>✘</td><td>✔</td></tr>
<tr><td>LiveQA</td><td>✘</td><td>-</td><td>✘</td><td>✘</td><td>✔</td><td>✔</td></tr>
<tr><td>ClinicBench</td><td>✘</td><td>-</td><td>✘</td><td>✔</td><td>✔</td><td>✔</td></tr>
<tr><td><strong>Ours</strong></td><td>✔</td><td>24</td><td>✔</td><td>✔</td><td>✔</td><td>✔</td></tr>
</tbody>
</table>

</body>
</html>
